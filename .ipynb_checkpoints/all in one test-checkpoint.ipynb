{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:06:17.600416Z",
     "start_time": "2018-01-15T14:05:52.759622Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import (Conv1D, LSTM, BatchNormalization, Flatten, Dense, Activation, \n",
    "                          Input, concatenate)\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.optimizers import Nadam\n",
    "from clr_callback import CyclicLR\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:06:17.713298Z",
     "start_time": "2018-01-15T14:06:17.603349Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev):\n",
    "    x = Conv1D(32, 1, kernel_regularizer=l2(10e-4),\n",
    "               bias_regularizer=l2(10e-4))(prev)\n",
    "    x = BatchNormalization(x)\n",
    "    x = concatenate([x, prev], axis=-1)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def res_layer(prev):\n",
    "    x = Conv1D(32, 1, kernel_regularizer=l2(10e-4),\n",
    "               bias_regularizer=l2(10e-4))(prev)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv1D(32, 1, kernel_regularizer=l2(\n",
    "        10e-4), bias_regularizer=l2(10e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = concatenate([x, prev], axis=-1)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def lstm_layer(prev):\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True, kernel_regularizer=l2(\n",
    "        10e-4), bias_regularizer=l2(10e-4)))(prev)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True, kernel_regularizer=l2(\n",
    "        10e-4), bias_regularizer=l2(10e-4)))(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def lstm_value_head(prev):\n",
    "    x = Bidirectional(LSTM(1, return_sequences=False, kernel_regularizer=l2(\n",
    "        10e-4), bias_regularizer=l2(10e-4)))(prev)\n",
    "    x = Dense(32)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dense(1, activation=\"tanh\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def lstm_policy_head(prev, output_length):\n",
    "    x = Bidirectional(LSTM(1, return_sequences=True, kernel_regularizer=l2(\n",
    "        10e-4), bias_regularizer=l2(10e-4)))(prev)\n",
    "    x = Bidirectional(LSTM(1, return_sequences=False, kernel_regularizer=l2(\n",
    "        10e-4), bias_regularizer=l2(10e-4)))(x)\n",
    "    x = Dense(output_length, activation=\"softmax\",\n",
    "              kernel_regularizer=l2(10e-4), bias_regularizer=l2(10e-4))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:06:17.817100Z",
     "start_time": "2018-01-15T14:06:17.716113Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, Nadam\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.initializers import glorot_uniform, zero\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def create_net(time_steps, input_length, output_length, num_layers=8):\n",
    "#     so the input is one current state\n",
    "    inp = Input(shape=(time_steps, input_length))\n",
    "#     I would have a series of mini heads that produce these outputs\n",
    "# or I could try to have an LSTM that remembers these values and outputs them\n",
    "\n",
    "# so at a high level what does MCTS take in and do\n",
    "# it takes in a node and a state\n",
    "# the node is a (s, a) pair\n",
    "# so given a (s, a) and it's \n",
    "# it's (s, a) because (s, a') will have different values\n",
    "# so given a (s, a), have a network that will predict probabilities and a result\n",
    "# \n",
    "\n",
    "\n",
    "# so you want a subsection of the network dedicated to being the true MCTS. for a given number of \n",
    "# simulations (1600), that section uses the (s, a) to produce new (s, a)'s (s_next, a_next)\n",
    "# \n",
    "\n",
    "\n",
    "# so what would be great is passing a bunch of input data and output data and having the MCTS infer everything\n",
    "# from that. So for example, take in an existing TSP program, have an LSTM that gets a set of inputs (num, num,\n",
    "# num, stop). for example for x y coords you would do num (x), num(y), stop\n",
    "# I would need to think about it more for more complicated examples. Maybe MaML will solve some of these issues\n",
    "\n",
    "# input and output need to be scaled between -1 to 1\n",
    "# maybe have a network that learns a rescale function \n",
    "\n",
    "# the output would then be how bad it is on a scale of distance(min_dist, min_dist)*n, \n",
    "# distance(max_dist, max_dist)*n, rescaled to -1 to 1\n",
    "\n",
    "# so now you would have an input and output\n",
    "\n",
    "# the next piece you need is \n",
    "\n",
    "# the issue with all of this is that it's a ton of work and it isn't guarenteed to be better \n",
    "\n",
    "# the main benefit would be if you could take the MCTS' prediction and improve it. (which is basically\n",
    "# what alphago did) \n",
    "    visits = Input(shape=())\n",
    "    total_value = Input(shape=(time_steps, input_length))\n",
    "    mean_value = Input(shape=(time_steps, input_length))\n",
    "    x = inp\n",
    "    for _ in range(num_layers):\n",
    "        x = lstm_layer(x)\n",
    "    policy = lstm_policy_head(x, output_length)\n",
    "    value = lstm_value_head(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=[policy, value])\n",
    "    \n",
    "    model.compile(optimizer=Nadam(), loss = [\"categorical_crossentropy\", \"mse\"], loss_weights = [.5, .5], \n",
    "                  metrics=None)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so for the lookback, if you go back to previous timesteps and evaluate branches from there, you could \n",
    "# \n",
    "\n",
    "# so if the value for a branch changes/is dramatically different from predicted,\n",
    "# you do more evaluations to previous time steps for example, \n",
    "# 400 on the last one, 200 on the one before, 100, 50, 25, 25 or something\n",
    "\n",
    "# this should result in additional computation being allocated on pivotol moments\n",
    "# could have a mixing term or parameter that determines what % is spent on reflection vs look ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the alpha zero go project and try to add concurrency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
