{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T18:35:52.492946Z",
     "start_time": "2018-01-15T18:35:52.481359Z"
    }
   },
   "outputs": [],
   "source": [
    "# So the article outlines three types of hierarchy\n",
    "\n",
    "# 1. Statistical learning. Given all of the past data points, what can we infer is the overall probability\n",
    "# distribution\n",
    "\n",
    "# 2. Incorportating decisions. Given a certain observation, what will my interaction do (lead by a goal)\n",
    "# Similar to reinforcement learning. Look ahead to available state, action pairs and see the value function \n",
    "# for those\n",
    "\n",
    "# 3. Causal Inference: What would have happened if I did this instead of that. This sounds like maybe a reverse\n",
    "# Value function. For example, what could I have done if I went back X timesteps and tried a different path\n",
    "# Might increase the learning speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T18:36:12.950754Z",
     "start_time": "2018-01-15T18:36:11.137603Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T18:36:12.959018Z",
     "start_time": "2018-01-15T18:36:12.955010Z"
    }
   },
   "outputs": [],
   "source": [
    "# So if you have a root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T18:36:20.528420Z",
     "start_time": "2018-01-15T18:36:20.519552Z"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T18:36:40.473221Z",
     "start_time": "2018-01-15T18:36:40.465894Z"
    }
   },
   "outputs": [],
   "source": [
    "G.add_node(\"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T18:37:06.821558Z",
     "start_time": "2018-01-15T18:37:06.817516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets say you have two actions, left or right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T18:38:13.555472Z",
     "start_time": "2018-01-15T18:38:13.545959Z"
    }
   },
   "outputs": [],
   "source": [
    "G.add_nodes_from([\"left\", \"right\"])\n",
    "G.add_edges_from([(\"root\", \"left\"), (\"root\", \"right\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T18:41:58.472001Z",
     "start_time": "2018-01-15T18:41:58.467110Z"
    }
   },
   "outputs": [],
   "source": [
    "# Going left lets say puts you in a dark forest path\n",
    "# based on a value function (neural net looks and sees that these colors are associated with danger)\n",
    "# the agent will be encouraged to turn back (added action for backtracking[wouldnt apply to sequential states,\n",
    "# such as Go]). This implies a greedy interaction however, if the other path is also bad, the agent may get \n",
    "# stuck alternating between them\n",
    "# so you need another look ahead mechanism, and predict how that path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T18:49:35.012938Z",
     "start_time": "2018-01-15T18:49:35.006020Z"
    }
   },
   "outputs": [],
   "source": [
    "# So MCTS with a backup/turn around action (non sequential) \n",
    "# So imagine an MCTS with a certain number of state buckets (1000 for example)\n",
    "# All states are connected to all other states\n",
    "# Certain sequences of actions form patterns \n",
    "# for example in a maze, you could be randomly dropped in\n",
    "# from there you can pick up on certain paths which tend to lead towards the goal (\"head west, avoid the trap\n",
    "# in the middle\")\n",
    "# These path patterns can play out from a random initialization or the agent picking the initial state somehow\n",
    "# for example, from the the terminal state of a sequence (well, if all states are connected by all actions,\n",
    "# the policy can just choose to go to a new state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T18:51:45.446278Z",
     "start_time": "2018-01-15T18:51:45.436136Z"
    }
   },
   "outputs": [],
   "source": [
    "# So for example a totally connected MCTS that has states equal to the number notes possible (all for a 24\n",
    "# fret electric guitar). The concept of time is very important to music, so the undirectedness creates some\n",
    "# issues. Two options are having the last couple states be used as input to the network, or having an LSTM\n",
    "# which gets the entire history (or to a certain point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T18:52:02.424331Z",
     "start_time": "2018-01-15T18:52:02.420027Z"
    }
   },
   "outputs": [],
   "source": [
    "# I'm not totally seeing the advantage of this over a transitional MCTS, it just seems like semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T18:53:38.193157Z",
     "start_time": "2018-01-15T18:53:38.189010Z"
    }
   },
   "outputs": [],
   "source": [
    "# The question is how to incorporate counterfactuals into MCTS. It seems like intervention is defined in\n",
    "# the look ahead for MCTS. It checks estimated results from current actions and has that inform the decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T19:04:19.836860Z",
     "start_time": "2018-01-15T19:04:19.819231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Counterfactuals seem to be mainly concerned about evaluating why something happened\n",
    "# So maybe after choosing an action, a break reverse lookahead (retrospective search) is performed\n",
    "# which compares some previous point in the path to the current value\n",
    "# so it should find a change point or point of fault\n",
    "# \"Oh, when I did that move 3 turns ago it dramatically lowered my chance of winning, but I didn't see until\n",
    "# this turn\"\n",
    "\n",
    "# so the goal of retrospective search is to compare the current value to the value of previous time steps\n",
    "# it identifies what point was most responsible for the reduction / increase in value\n",
    "\n",
    "# so the question remains of how do you train this retrospective search\n",
    "# you could evaluate trendlines and see when the value starts going up/down\n",
    "# \n",
    "\n",
    "\n",
    "# so the goal of retrospective search is finding turning points\n",
    "# so given the history (all or up to a certain point) what was the pivotol moment that led to this current value\n",
    "\n",
    "# that can be found using a neural network, whose goal is to find at which indexed timestep that the value\n",
    "# significantly changes. So maybe sequentially (bidirectional could work), go from all of the states, front\n",
    "# to back, revealing one at a time. Have the network match the trendline of this progression\n",
    "# Given that trendline, identify the points that lead to sudden shifts in the value trend\n",
    "# That wouldnt account for delayed reactions however. \n",
    "# It gets very hard to map long term causes and effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T19:05:50.864304Z",
     "start_time": "2018-01-15T19:05:50.859349Z"
    }
   },
   "outputs": [],
   "source": [
    "# So imagine that you have a perfect model that finds what the pivotol points in history are\n",
    "# what do you do with that information?\n",
    "# in theory you want to give more weight to those decisions and probably explore them more thoroughly\n",
    "# for example, for pivot points, they get more simulations than non pivot points\n",
    "# that should in theory give more weight to those important moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T19:06:10.461596Z",
     "start_time": "2018-01-15T19:06:10.457371Z"
    }
   },
   "outputs": [],
   "source": [
    "# So that sounds reasonable, now how would we have a perfect model that finds pivotol points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T19:20:01.466674Z",
     "start_time": "2018-01-15T19:20:01.455176Z"
    }
   },
   "outputs": [],
   "source": [
    "# training a network to identify it is a good first step. \n",
    "# Given a certain history of actions, predict for each whether or not it is a pivot point\n",
    "\n",
    "# could possibly pretrain on toy examples where the pivot point is known ahead of time, and let the network find\n",
    "# it\n",
    "\n",
    "# so what is the overall benefit of this whole idea\n",
    "# what it should do is focus the search more on the key moments, and ignore/lessen the exploration of less \n",
    "# important points. A key goal that we would want is that as the number of MCTS iterations go to infinity,\n",
    "# you must be able to visit every point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T19:47:08.482871Z",
     "start_time": "2018-01-15T19:47:08.478566Z"
    }
   },
   "outputs": [],
   "source": [
    "# how could I put all of alphazero inside of a neural net?\n",
    "\n",
    "# I could have a lot of external losses which control how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Association P(y|x) \n",
    "# Markov chains. and MRP?\n",
    "# What is the chance I transition from state0 to state1\n",
    "# what is the expected reward with this policy\n",
    "# What is the chance of winning, given this board state\n",
    "\n",
    "# 2. Intervention\n",
    "# P(y|do(x), z)\n",
    "# Look ahead, MCTS simulations\n",
    "# What if I go right => run n simulations, use updated policy to proceed\n",
    "\n",
    "# 3. Counterfactuals\n",
    "# P(y_x|x_prime, y_prime)\n",
    "# Imagining, retrospection\n",
    "\n",
    "# Why did that action improve my chance of winning\n",
    "# It was because this pattern in the state changed\n",
    "# The neural net finds these patterns and differentiates states\n",
    "# Was it state Xt-1 that caused Y or was it Xt-2, Xt-3, etc\n",
    "# Finding pivotol moments in the history, the important information\n",
    "\n",
    "# So maybe two components are finding what previous (s, a) most directly correlates\n",
    "# this one\n",
    "# after finding which most directly correlates, you want to investigate that branch\n",
    "# a lot more\n",
    "\n",
    "# so maybe, look with an exponentially smoothing average # of simulations for the \n",
    "# history that we care about, and see what state most consistently leads to this \n",
    "# level of value. \n",
    "\n",
    "# after identifying the most important previous step, run additional simulations \n",
    "# over it to maximally explore it\n",
    "\n",
    "# would the interaction with the neural net need to be changed? \n",
    "# You could have a neural net that identifies which previous step most directly\n",
    "# caused the value of this one. \n",
    "\n",
    "# so for example find a brute force way of running many simulations over the parents\n",
    "# and see which one has average values most similar to the current one, and then\n",
    "# give that result to a state connector head\n",
    "\n",
    "# but maybe instead of that you want to have a neural net that predicts how important\n",
    "# this state is. depending on how important this current state is you can spend\n",
    "# more time on retrospection or lookahead\n",
    "\n",
    "# so maybe the mix between lookahead and retrospection is defined by this neural net\n",
    "# which predicts how pivotol this current state is\n",
    "\n",
    "# pivot points which are found by running many simulations on parents\n",
    "# after a signicant value change are marked as 1's, the rest are -1's or 0's\n",
    "\n",
    "# do you want to differentiate between good pivot points and bad pivot points?\n",
    "# if a certain action led to a really good state or a really bad state, you probably\n",
    "# want to reflect on it a lot more. in that cause you just care about whether it is\n",
    "# a turning point\n",
    "\n",
    "#  focusing simulations on lookahead probably will always be better at game time, \n",
    "# since you always want to choose the strongest, most well thought out move\n",
    "\n",
    "# however, the reflection would be a key point for training the algorithm\n",
    "# it would allow the type of things we see with professional teams that evaluate\n",
    "# what the other team did and adjust their gameplay accordingly\n",
    "\n",
    "# Does that type of reflection fall under the same category as the previous look back?\n",
    "\n",
    "# I think not. I think that you could imagine a situation where you play out an example\n",
    "# game from a human player or a different algorithm (or the same algorithm in the past)\n",
    "# and it evaluates each position and again identifies key moments.\n",
    "# key moments get additional simulation time\n",
    "\n",
    "# so this would for example allow an MCTS to look at the behaviors of a certain opponent\n",
    "# and learn counters to it. It might cause overfitting / increase exploitability,\n",
    "# but it also would in theory increase the personalization and allow to potentially\n",
    "# better strategies for specific people/players\n",
    "\n",
    "# so to summarize, the key idea I have is identifying how important certain moments are\n",
    "# and choosing the resource trade off between retrospection (reflection on those\n",
    "# turning points in the form of additional simulations) and lookahead (running simulations\n",
    "# to see what the best action is from this point).\n",
    "# at performance / test time, it should be 100% lookahead, but at train time the ratio\n",
    "# is not obvious. if you are keeping track of a long term MCTS, maybe pure reflection\n",
    "# is superior, as it should focus simulations on moments that were more influential\n",
    "# on the state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
